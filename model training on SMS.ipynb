{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Install required libraries\n", "!pip install transformers torch scikit-learn"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import torch\n", "from torch.utils.data import TensorDataset, DataLoader\n", "from torch.nn import CrossEntropyLoss\n", "from torch.optim import AdamW\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.utils.class_weight import compute_class_weight\n", "from transformers import MobileBertTokenizer, MobileBertForSequenceClassification"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load your SMS dataset\n", "df = pd.read_csv('/content/sms_spam.csv')  # change to your dataset path\n", "\n", "# Label encoding\n", "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n", "\n", "# Split\n", "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Tokenizer + Model\n", "tokenizer = MobileBertTokenizer.from_pretrained(\"google/mobilebert-uncased\")\n", "model = MobileBertForSequenceClassification.from_pretrained(\"google/mobilebert-uncased\", num_labels=2)\n", "\n", "# Class weights\n", "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n", "weights = torch.tensor(class_weights, dtype=torch.float).to(model.device)\n", "loss_fn = CrossEntropyLoss(weight=weights)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Tokenize\n", "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True)\n", "train_dataset = TensorDataset(\n", "    torch.tensor(train_encodings['input_ids']),\n", "    torch.tensor(train_encodings['attention_mask']),\n", "    torch.tensor(y_train.values)\n", ")\n", "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n", "\n", "# Optimizer\n", "optimizer = AdamW(model.parameters(), lr=2e-5)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Training loop\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "model.to(device)\n", "model.train()\n", "for epoch in range(2):\n", "    for batch in train_dataloader:\n", "        input_ids = batch[0].to(device)\n", "        attention_mask = batch[1].to(device)\n", "        labels = batch[2].to(device)\n", "\n", "        optimizer.zero_grad()\n", "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n", "        logits = outputs.logits\n", "\n", "        loss = loss_fn(logits, labels)\n", "        loss.backward()\n", "        optimizer.step()\n", "\n", "    print(f\"Epoch {epoch+1} Loss: {loss.item()}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Example SMS prediction\n", "def predict_sms(text):\n", "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n", "    inputs = {k: v.to(device) for k, v in inputs.items()}\n", "    model.eval()\n", "    with torch.no_grad():\n", "        outputs = model(**inputs)\n", "        pred = torch.argmax(outputs.logits, dim=1).item()\n", "    return \"SPAM\" if pred == 1 else \"HAM\"\n", "\n", "# Example test\n", "print(predict_sms(\"Congratulations! You've won a free ticket. Reply now!\"))\n", "print(predict_sms(\"Hey, are you coming to class tomorrow?\"))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Save model & tokenizer\n", "model.save_pretrained(\"./fine_tuned_mobilebert_sms/\")\n", "tokenizer.save_pretrained(\"./fine_tuned_mobilebert_sms/\")\n", "!zip -r mobilebert_sms_model.zip fine_tuned_mobilebert_sms"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 2}